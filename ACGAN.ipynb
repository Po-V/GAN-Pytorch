{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a838cb0",
   "metadata": {},
   "source": [
    "Auxiliary GAN is similar to conditional GAN. The difference is that in ACGAN, the discriminator does not receive input of class labels. Instead it makes predictions of class labels of image and also weather an image is real or fake without receiving the labels as input. The generator in ACGAN is provided with both the labels and latent space points.\n",
    "\n",
    "The generator is trying to maximise the log-likelihood of prediction of discriminator of the correct class (LC) and minimise the log-likelihood in ability of discriminator in detecting real and fake images (LS). The discriminator is trying to maximise both LS and LC. \n",
    "\n",
    "Discriminator : LS + LC\n",
    "\n",
    "Generator     : LC - LS\n",
    "\n",
    "This causes AC-GANs learn a representation for latent space that is independent of class label. This results in higher quality samples and more stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfff683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a62ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 110, img_channels = 1, hidden_dim = 64):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.z_dim = z_dim\n",
    "        self.emb = nn.Embedding(10, 10)\n",
    "        \n",
    "        self.gen = nn.Sequential(\n",
    "            self.gen_block(z_dim, hidden_dim*16, 4, 1, 0),\n",
    "            self.gen_block(hidden_dim*16, hidden_dim*8, 4, 2, 1),\n",
    "            self.gen_block(hidden_dim*8, hidden_dim*4, 4, 2, 1),\n",
    "            self.gen_block(hidden_dim*4, hidden_dim*2, 4, 2, 1),\n",
    "            nn.ConvTranspose2d(hidden_dim*2, 1, kernel_size = 4, stride = 2, padding = 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def gen_block(self, input_channels, output_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding, bias = False),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, noise, labels):\n",
    "        embedding = self.emb(labels)\n",
    "        embedding_noise = torch.cat((noise, embedding), 1)\n",
    "        embedding_noise = embedding_noise.view(-1, self.z_dim, 1, 1)\n",
    "        img = self.gen(embedding_noise)\n",
    "        return img.view(img.size(0), 1, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4024788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71600b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channel = 1, hidden_dim = 64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Conv2d(img_channel, hidden_dim, kernel_size = 4, stride = 2, padding = 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self.disc_block(hidden_dim, hidden_dim*2, 4, 2, 1),\n",
    "            self.disc_block(hidden_dim*2, hidden_dim*4, 4, 2, 1),\n",
    "            self.disc_block(hidden_dim*4, hidden_dim*8, 4, 2, 1),\n",
    "#             nn.Conv2d(hidden_dim*8, 1, kernel_size = 4, stride = 2, padding = 0),    \n",
    "        )\n",
    "        \n",
    "        self.label = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim*8, 1, kernel_size = 4, stride = 2, padding = 0),    \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim*8, 10, kernel_size = 4, stride = 2, padding = 0),    \n",
    "            nn.LogSoftmax(dim = 1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def disc_block(self, input_channels, output_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(input_channels, output_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        disc_maps = self.disc(img)\n",
    "        label = self.label(disc_maps)\n",
    "        classes = self.classifier(disc_maps)\n",
    "        return label, classes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d8ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02),\n",
    "        torch.nn.init.normal_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a24f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "z_dim = 100\n",
    "batch_size = 128\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "lr = 0.0001\n",
    "image_size = 64\n",
    "device = 'cuda'\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion2 = nn.NLLLoss()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    FashionMNIST(\".\", download = False, transform = transform),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0af8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator().to(device)\n",
    "disc = Discriminator().to(device)\n",
    "\n",
    "weights_init(gen)\n",
    "weights_init(disc)\n",
    "\n",
    "opt_gen = torch.optim.Adam(gen.parameters(), lr = lr, betas = (beta_1, beta_2))\n",
    "opt_disc = torch.optim.Adam(disc.parameters(), lr = lr, betas = (beta_1, beta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f83825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    img = img.detach().cpu()\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b5561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_step = 0\n",
    "gen_losses = 0\n",
    "disc_losses = 0\n",
    "steps = 100\n",
    "os.makedirs('ACGAN', exist_ok = True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for real, labels in(dataloader):\n",
    "        real_imgs = Variable(real.type(torch.FloatTensor)).to(device)\n",
    "        labels = Variable(labels.type(torch.LongTensor)).to(device)\n",
    "        \n",
    "        opt_disc.zero_grad()\n",
    "        fake_noise = get_noise(len(real), z_dim)\n",
    "        fake_labels = Variable(torch.LongTensor(np.random.randint(0, 10, len(real)))).to(device)\n",
    "        fake = gen(fake_noise, fake_labels)\n",
    "        fake_preds_labels, fake_preds_classes = disc(fake.detach())\n",
    "#         print(fake_preds_labels.shape) [128, 1, 1, 1]\n",
    "#         print(fake_preds_classes.shape) [128, 10, 1, 1]\n",
    "        fake_preds_labels = fake_preds_labels.view(-1, 1)\n",
    "        fake_preds_classes = fake_preds_classes.view(-1, 10)\n",
    "        disc_fakelabel_loss = criterion(fake_preds_labels, torch.zeros_like(fake_preds_labels))\n",
    "        disc_fakeclass_loss = criterion2(fake_preds_classes, fake_labels)\n",
    "        disc_fake_loss = (disc_fakelabel_loss + disc_fakeclass_loss) / 2\n",
    "        \n",
    "        real_preds_labels, real_preds_classes = disc(real_imgs)\n",
    "        real_preds_labels = real_preds_labels.view(-1, 1)\n",
    "        real_preds_classes = real_preds_classes.view(-1, 10)\n",
    "        disc_reallabel_loss = criterion(real_preds_labels, torch.ones_like(real_preds_labels))\n",
    "        disc_realclass_loss = criterion2(real_preds_classes, labels)\n",
    "        disc_real_loss = (disc_reallabel_loss + disc_realclass_loss) / 2\n",
    "        \n",
    "        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "        disc_loss.backward(retain_graph = True)\n",
    "        opt_disc.step()\n",
    "        disc_losses += disc_loss.item()\n",
    "\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        fake_noise_2 = get_noise(len(real), z_dim)\n",
    "        fake_labels_2  = Variable(torch.LongTensor(np.random.randint(0, 10, len(real)))).to(device)\n",
    "        fake_2 = gen(fake_noise_2, fake_labels_2)\n",
    "        fake_preds_labels, fake_preds_classes = disc(fake_2)\n",
    "        fake_preds_labels = fake_preds_labels.view(-1, 1)\n",
    "        fake_preds_classes = fake_preds_classes.view(-1, 10)\n",
    "        disc_fakelabel_loss = criterion(fake_preds_labels, torch.ones_like(fake_preds_labels))\n",
    "        disc_fakeclass_loss = criterion2(fake_preds_classes, fake_labels_2)\n",
    "        gen_loss = (disc_fakelabel_loss + disc_fakeclass_loss) / 2\n",
    "        gen_loss.backward()\n",
    "        opt_gen.step()\n",
    "        gen_losses += gen_loss.item()\n",
    "        \n",
    "        if current_step % steps == 0 and current_step > 0:\n",
    "            print(f\"Epochs: {epoch} Step: {current_step} Generator loss: {gen_losses / steps}, discriminator loss: {disc_losses / steps}\")\n",
    "            img_grid_real = torchvision.utils.make_grid(real[:25], nrow = 5)\n",
    "            img_grid_fake = torchvision.utils.make_grid(fake[:25], nrow = 5)\n",
    "            save_image(fake_2.data[:36], 'ACGAN/%d.png' % current_step, nrow = 6, normalize = True)\n",
    "            matplotlib_imshow(img_grid_real, one_channel = True)\n",
    "            matplotlib_imshow(img_grid_fake, one_channel = True)\n",
    "            gen_losses = 0\n",
    "            disc_losses = 0\n",
    "        \n",
    "        current_step += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66b6c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
