{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28c238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image\n",
    "import torchvision\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ab8625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.resnet = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(64)          \n",
    "        )\n",
    "        self.relu = nn.PReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip_connect = x\n",
    "        x = skip_connect + self.resnet(x)\n",
    "        return self.relu(x)\n",
    "        \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels = 3, hidden_channels = 64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size = 9, stride =1, padding = 4),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            *[ResNet(64) for _ in range(16)]\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        \n",
    "        self.layer_up = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        self.final_layer = nn.Conv2d(64, 3, kernel_size = 9, stride = 1, padding = 4) \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, img):\n",
    "        img1 = self.layer1(img)\n",
    "        x = self.layer2(img1)\n",
    "        x = self.layer3(x) + img1\n",
    "#         print(x.shape)\n",
    "        x = self.layer_up(x)\n",
    "        x = self.layer_up(x)\n",
    "#         print(x.shape)\n",
    "        x = self.final_layer(x)\n",
    "        return self.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d37ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    imgs_fake = torch.randn((1, 3, 64, 64))\n",
    "    model = Generator()\n",
    "    preds = model(imgs_fake)\n",
    "    print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcf8dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29451767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels = 3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.relu1 = nn.LeakyReLU(0.2)\n",
    "        con_blocks = []\n",
    "        con_blocks.append(self.con_block(64, 64, 3, 2, 1))\n",
    "        con_blocks.append(self.con_block(64, 128, 3, 1, 1))\n",
    "        con_blocks.append(self.con_block(128, 128, 3, 2, 1))\n",
    "        con_blocks.append(self.con_block(128, 256, 3, 1, 1))\n",
    "        con_blocks.append(self.con_block(256, 256, 3, 2, 1))\n",
    "        con_blocks.append(self.con_block(256, 512, 3, 1, 1))\n",
    "        con_blocks.append(self.con_block(512, 512, 3, 2, 1))\n",
    "        self.seq = nn.Sequential(*con_blocks)\n",
    "        self.linear1 = nn.Linear(512*16*16, 1024) # 512*6*6\n",
    "        self.relu2 = nn.LeakyReLU(0.2)\n",
    "        self.linear2 = nn.Linear(1024, 1)\n",
    "#         self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def con_block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        x = self.conv1(img)\n",
    "        x = self.relu1(x)\n",
    "        x = self.seq(x)\n",
    "#         print(x.shape)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.linear2(x)\n",
    "#         x =  self.sig(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8c9790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    imgs_fake = torch.randn((1, 3, 256, 256))\n",
    "    model = Discriminator()\n",
    "    preds = model(imgs_fake)\n",
    "    print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5142dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02),\n",
    "        torch.nn.init.normal_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d9461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir_lr, root_dir_hr, lr_transform = None, hr_transform = None):\n",
    "        self.root_dir_lr = root_dir_lr\n",
    "        self.root_dir_hr = root_dir_hr\n",
    "        self.lr_files = os.listdir(self.root_dir_lr)\n",
    "        self.hr_files = os.listdir(self.root_dir_hr)\n",
    "        self.lr_transform = lr_transform\n",
    "        self.hr_transform = hr_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        lr_img = self.lr_files[index % len(self.lr_files)]\n",
    "        hr_img = self.hr_files[index % len(self.hr_files)]\n",
    "        lr_img_path = os.path.join(self.root_dir_lr, lr_img)\n",
    "        hr_img_path = os.path.join(self.root_dir_hr, hr_img)\n",
    "        img_lr = Image.open(lr_img_path)\n",
    "        img_hr = Image.open(hr_img_path)\n",
    "\n",
    "        if self.lr_transform or self.hr_transform :\n",
    "            img_lr = self.lr_transform(img_lr)\n",
    "            img_hr = self.hr_transform(img_hr)\n",
    "\n",
    "        return img_lr, img_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6a1aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_lr = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transforms_hr = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset = ImageDataset(\"Data/LR/\", \"Data/HR/\", lr_transform=transforms_lr, hr_transform = transforms_hr)\n",
    "dataloader = DataLoader(dataset, batch_size= 16, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e1e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for z, h in dataloader:\n",
    "#         print(z.shape)\n",
    "#         save_image(z, \"z.png\")\n",
    "#         save_image(h, \"h.png\")\n",
    "#         break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab3a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg19\n",
    "\n",
    "class VGGFeatures(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGFeatures, self).__init__()\n",
    "        self.vgg19 = vgg19(pretrained = True).features[:18].eval().to('cpu')\n",
    "        \n",
    "        for param in self.vgg19.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, img):\n",
    "        return self.vgg19(img)\n",
    "    \n",
    "vgg = VGGFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab3dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "lr = 1e-4\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "n_epochs = 100\n",
    "img_channels = 3\n",
    "steps = 6\n",
    "batch_size = 16\n",
    "\n",
    "gen = Generator().to(device)\n",
    "disc = Discriminator().to(device)\n",
    "\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr = lr, betas = (beta_1, beta_2))\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr = lr, betas = (beta_1, beta_2))\n",
    "gen = gen.apply(weights_init)\n",
    "disc = disc.apply(weights_init)\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "bce = nn.BCEwithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4ea863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    img = img.detach().cpu()\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa2b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_step = 0\n",
    "gen_losses = 0\n",
    "disc_losses = 0\n",
    "os.makedirs(\"srgan\", exist_ok=True)\n",
    "\n",
    "for epochs in range(n_epochs):\n",
    "    for lr, hr in dataloader:\n",
    "        lr = lr.to(device)\n",
    "        hr = hr.to(device)\n",
    "        current_batch_size = len(lr)\n",
    "        \n",
    "        disc_opt.zero_grad()\n",
    "        fake_hr = gen(lr)\n",
    "        disc_preds_fake = disc(fake_hr.detach())\n",
    "        disc_fake_loss = bce(disc_preds_fake, torch.zeros_like(disc_preds_fake))\n",
    "        disc_preds_real = disc(hr)\n",
    "        disc_real_loss = bce(disc_preds_real, torch.ones_like(disc_preds_real))\n",
    "        disc_loss = (disc_fake_loss + disc_real_loss ) / 2\n",
    "        disc_loss.backward(retain_graph = True)\n",
    "        disc_opt.step()\n",
    "        disc_losses += disc_loss.item()\n",
    "        \n",
    "        gen_opt.zero_grad()\n",
    "        fake_hr2 = gen(lr)\n",
    "        disc_preds_fake = disc(fake_hr2)\n",
    "        adv_loss = bce(disc_preds_fake, torch.ones_like(disc_preds_fake))\n",
    "        vgg_loss = mse(vgg(hr), vgg(fake_hr2))\n",
    "        perpectual_loss = vgg_loss + 1e-3*adv_loss\n",
    "        perpectual_loss.backward()\n",
    "        gen_opt.step()\n",
    "        gen_losses += perpectual_loss.item()\n",
    "        \n",
    "        if current_step % steps == 0 and current_step > 0 :\n",
    "            print(f\"Epochs: {epochs} Step: {current_step} Generator loss: {gen_losses / steps}, discriminator loss: {disc_losses / steps}\")\n",
    "            img_grid_hr = torchvision.utils.make_grid(torch.cat([hr[:1], fake_hr2[:1]]), nrow = 2)\n",
    "            save_image(fake_hr2.data[:3], \"srgan/%d.png\" % current_step, nrow=3, normalize=True)\n",
    "            matplotlib_imshow(img_grid_hr, one_channel=False)\n",
    "            gen_losses = 0\n",
    "            disc_losses = 0\n",
    "        \n",
    "        current_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb2ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
