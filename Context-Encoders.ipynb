{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1659b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import torchvision\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8badac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size = 4, stride = 2, padding = 1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Conv2d(64, 64, kernel_size = 4, stride = 2, padding = 1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Conv2d(64, 128, kernel_size = 4, stride = 2, padding = 1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Conv2d(128, 256, kernel_size = 4, stride = 2, padding = 1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Conv2d(256, 512, kernel_size = 4, stride = 2, padding = 1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.channel_wise = nn.Conv2d(512, 4000, kernel_size = 3)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "                nn.ConvTranspose2d(4000, 512, kernel_size = 4, stride =2, padding = 1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(512, 256, kernel_size = 4, stride = 2, padding = 1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(256, 128, kernel_size = 4, stride = 2, padding = 1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(128, 64, kernel_size = 4, stride = 2, padding = 1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(64, 3, kernel_size = 4, stride = 2, padding = 1),\n",
    "                nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.channel_wise(x)\n",
    "        return self.decoder(x)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc24fc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    imgs = torch.randn((1, 3, 128, 128))\n",
    "    gen = Generator()\n",
    "    preds = gen(imgs)\n",
    "    print(preds.shape)\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ecbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size = 4, stride = 2, padding = 1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Conv2d(64, 128, kernel_size = 4, stride = 2, padding = 1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Conv2d(128, 256, kernel_size = 4, stride = 2, padding = 1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Conv2d(256, 512, kernel_size = 4, stride = 2, padding = 1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Conv2d(512, 1, kernel_size = 4, stride = 2, padding = 0),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae1c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    imgs = torch.randn((1, 3, 64, 64))\n",
    "    disc = Discriminator()\n",
    "    preds = disc(imgs)\n",
    "    print(preds.shape)\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02),\n",
    "        torch.nn.init.normal_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f7cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion1 = nn.BCELoss()\n",
    "criterion2 = nn.MSELoss()\n",
    "\n",
    "n_epochs = 20\n",
    "img_channels = 3\n",
    "lr = 0.0002\n",
    "device = 'cuda'\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd33ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator().to(device)\n",
    "disc = Discriminator().to(device)\n",
    "\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr = lr, betas = (beta_1, beta_2))\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr = lr, betas = (beta_1, beta_2))\n",
    "\n",
    "gen = gen.apply(weights_init)\n",
    "disc = disc.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021f9cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform = None):\n",
    "        self.root_dir = root_dir\n",
    "        self.list_files = os.listdir(self.root_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_files)\n",
    "    \n",
    "    def mask(self, img):\n",
    "#         x, y = np.random.randint(0, 64, 2)\n",
    "#         x1, y1 = x+64, y+64\n",
    "#         masked_part = img[:, x:x1, y:y1]\n",
    "#         masked_img = img.clone()\n",
    "#         masked_img[:, x:x1, y:y1] = 1\n",
    "\n",
    "        x = (128 - 64) // 2\n",
    "        masked_img = img.clone()\n",
    "        masked_part = img[:, x:x+64, x: x+64]\n",
    "        masked_img[:, x: x+64, x:x+64] = 1\n",
    "        return masked_part, masked_img\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_file = self.list_files[index]\n",
    "        img_path = os.path.join(self.root_dir, img_file)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform :\n",
    "            image = self.transform(image)\n",
    "        masked_part, masked_img = self.mask(image)\n",
    "        return image, masked_part, masked_img\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434737f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset = ImageDataset('../horse2zebra/trainB/', transform = transform)\n",
    "dataloader = DataLoader(dataset, batch_size = 16)\n",
    "\n",
    "# for img, mask_part, mask_img in dataloader:\n",
    "#         print(img.shape)\n",
    "#         print(mask_part.shape)\n",
    "#         save_image(img, \"sat.png\")\n",
    "#         save_image(mask_part, \"real.png\")\n",
    "#         save_image(mask_img, \"real2.png\")\n",
    "#         break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027a6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    img = img.detach().cpu()\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c642f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_step = 0\n",
    "gen_losses = 0\n",
    "disc_losses = 0\n",
    "# os.makedirs(\"context-encoders\", exist_ok=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for img, img_mask, masked_img in dataloader:\n",
    "        img = img.to(device)\n",
    "        img_mask = img_mask.to(device)\n",
    "        masked_img = masked_img.to(device)\n",
    "        \n",
    "        disc_opt.zero_grad()\n",
    "        fake_img_mask = gen(masked_img)\n",
    "        disc_preds_fake = disc(fake_img_mask.detach())\n",
    "        disc_fake_loss = criterion1(disc_preds_fake, torch.zeros_like(disc_preds_fake))\n",
    "        disc_fake_loss.backward()\n",
    "        disc_preds_real = disc(img_mask)\n",
    "        disc_real_loss = criterion1(disc_preds_real, torch.ones_like(disc_preds_real))\n",
    "        disc_real_loss.backward()\n",
    "        disc_loss = disc_fake_loss + disc_real_loss\n",
    "#         disc_loss.backward()\n",
    "        disc_opt.step()\n",
    "        disc_losses += disc_loss.item()\n",
    "        \n",
    "        gen_opt.zero_grad()\n",
    "        fake_img_mask2 = gen(masked_img)\n",
    "        gen_preds_fake = disc(fake_img_mask2)\n",
    "        gen_fake_loss = criterion1(gen_preds_fake, torch.ones_like(gen_preds_fake))\n",
    "        gen_mse_loss = criterion2(fake_img_mask2, img_mask)\n",
    "        gen_loss = (1-0.999)*gen_fake_loss + 0.999*gen_mse_loss\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "        gen_losses += gen_loss.item()\n",
    "\n",
    "                \n",
    "        if current_step % steps == 0 and current_step > 0 :\n",
    "            print(f\"Epochs: {epoch} Step: {current_step} Generator loss: {gen_losses / steps}, discriminator loss: {disc_losses / steps}\")\n",
    "            img_grid_real = torchvision.utils.make_grid(img[:5], nrow = 5)\n",
    "            img_grid_mask = torchvision.utils.make_grid(masked_img[:5], nrow = 5)\n",
    "            mask_img_gen = mask_img.clone()\n",
    "            mask_img_gen[:5, :, 32:32+64, 32:32+64] = fake_img_mask2.data[:5]\n",
    "            img_grid_fake = torchvision.utils.make_grid(fake_img_mask2.data[:5], nrow = 5)\n",
    "            matplotlib_imshow(img_grid_real, one_channel=False)\n",
    "            matplotlib_imshow(img_grid_mask,  one_channel=False)\n",
    "            matplotlib_imshow(img_grid_fake,  one_channel=False)\n",
    "            gen_losses = 0\n",
    "            disc_losses = 0\n",
    "        \n",
    "        current_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14988006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
